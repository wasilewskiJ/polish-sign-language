% Raport naukowy - Klasyfikacja PSL
\documentclass[runningheads]{llncs}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[polish]{babel}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{listings}
\usepackage{xcolor}

\lstset{
    basicstyle=\ttfamily\footnotesize,
    breaklines=true,
    frame=single,
    language=Python,
    showstringspaces=false
}

\begin{document}

\title{Klasyfikacja liter polskiego języka migowego - porównanie metod uczenia maszynowego z wykorzystaniem punktów charakterystycznych dłoni}
\titlerunning{Klasyfikacja PJM}

\author{Jakub Wasilewski, Igor Włodarczyk}

\authorrunning{J. Wasilewski, I. Włodarczyk}

\institute{Politechnika Wrocławska, Wydział Informatyki i Telekomunikacji\\
Nazwa kursu: Uczenie Maszynowe\\
Prowadzący: mgr. Szymon Wojciechowski}

\maketitle

\begin{abstract}
W pracy przedstawiono eksperymentalne porównanie metod klasyfikacji statycznych gestów polskiego języka migowego (PJM). Głównym celem badania było porównanie skuteczności różnych klasyfikatorów w rozpoznawaniu liter alfabetu PJM. Zbadano cztery podejścia: Random Forest, regresję logistyczną, wielowarstwową sieć neuronową oraz konwolucyjną sieć neuronową. Jako reprezentację danych wykorzystano 78-wymiarowy wektor cech ekstrahowanych z punktów charakterystycznych dłoni przy użyciu biblioteki MediaPipe. Przeprowadzono eksperymenty z i bez augmentacji danych na zbiorze 2549 próbek obejmujących 22 litery alfabetu PJM. Najlepsze wyniki uzyskał Random Forest, z dokładnością 98.87\% przy użyciu augmentacji danych. Praca zawiera szczegółową analizę metryk, macierzy pomyłek oraz wpływu augmentacji na wydajność modeli.

\keywords{język migowy \and klasyfikacja gestów \and uczenie maszynowe \and MediaPipe \and augmentacja danych}
\end{abstract}

\section{Wstęp}

Polski język migowy (PJM) stanowi naturalny środek komunikacji osób niesłyszących w Polsce. Rozpoznawanie gestów języka migowego za pomocą technik wizji komputerowej i uczenia maszynowego może znacząco ułatwić komunikację między osobami słyszącymi a niesłyszącymi, stanowiąc podstawę systemów tłumaczących w czasie rzeczywistym.

Niniejsza praca koncentrowała się na klasyfikacji statycznych gestów alfabetu PJM z wykorzystaniem cech ekstrahowanych z punktów charakterystycznych dłoni. W przeciwieństwie do podejść wykorzystujących bezpośrednio obrazy RGB, użycie punktów charakterystycznych znacząco redukowało wymiarowość problemu (z $224 \times 224 \times 3 = 150528$ do 78 cech), co pozwalało na bardziej efektywne trenowanie klasyfikatorów przy ograniczonej liczbie danych treningowych.

\subsection{Cel pracy}

Głównym celem badania było eksperymentalne porównanie skuteczności różnych klasyfikatorów uczenia maszynowego w zadaniu klasyfikacji liter PJM. Praca miała odpowiedzieć na pytanie, które z metod najlepiej radziły sobie z tym zadaniem, oraz jaki wpływ miała augmentacja danych na wydajność poszczególnych modeli. Dodatkowo weryfikowano, czy proste klasyfikatory mogły konkurować z głębokimi sieciami neuronowymi przy wykorzystaniu odpowiednio zaprojektowanych cech.

\section{Przegląd literatury}

Rozpoznawanie języka migowego jest aktywnie badanym obszarem na styku wizji komputerowej i uczenia maszynowego \cite{lecun1998,krizhevsky2012}. Większość prac koncentruje się na amerykańskim języku migowym (ASL), podczas gdy badania nad polskim językiem migowym są stosunkowo nieliczne.

Współczesne podejścia do rozpoznawania gestów można podzielić na dwie główne kategorie: metody oparte na bezpośredniej analizie obrazów oraz metody wykorzystujące punkty charakterystyczne (ang. \textit{hand landmarks}). Drugie podejście, popularyzowane przez bibliotekę MediaPipe, wykazuje wysoką efektywność przy znacznie mniejszej złożoności obliczeniowej.

W kontekście klasyfikacji języków migowych, Ameen i Vadera \cite{ameen2017} zastosowali CNN do klasyfikacji gestów ASL z obrazów głębi i RGB. Oguntimilehin i Balogun \cite{oguntimilehin2024} przedstawiają system rozpoznawania Nigerian Sign Language w czasie rzeczywistym, osiągając dokładność testową powyżej 90\%. Kołodziej i in. \cite{kolodziej2022} opisują system wykorzystujący architekturę DenseNet do rozpoznawania 24 znaków alfabetu ASL, osiągając dokładność bliską 100\% dla użytkowników widzianych podczas treningu oraz 83-85\% dla nowych użytkowników.

W kontekście polskiego języka migowego, Piskozub i Strumiłło \cite{piskozub2023} proponują rękawicę z czujnikami do rozpoznawania liter alfabetu PJM. Filipowska i in. \cite{filipowska2024} prezentują system rozpoznawania gestów PJM na podstawie sygnałów EMG, osiągając wysoką dokładność klasyfikacji. Brak jest jednak otwartoźródłowych projektów wykorzystujących standardowe kamery RGB do klasyfikacji statycznych liter PJM.

Podejścia oparte na punktach charakterystycznych dłoni oferują następujące zalety:
\begin{itemize}
    \item Niezmienniczość względem tła i warunków oświetleniowych
    \item Znacząca redukcja wymiarowości danych
    \item Możliwość przetwarzania w czasie rzeczywistym
    \item Łatwiejsza interpretacja modelu
\end{itemize}

Augmentacja danych w kontekście rozpoznawania gestów jest standardową techniką zwiększającą rozmiar zbioru treningowego poprzez transformacje geometryczne. W literaturze wykazano, że augmentacja znacząco poprawia generalizację modeli, szczególnie przy ograniczonej liczbie próbek treningowych.

\section{Zbiór danych}

\subsection{Metodologia zbierania danych}

Zbiór danych został ręcznie zebrany przy użyciu kamery internetowej i skryptu \texttt{collect\_dataset.py}. Proces zbierania obejmował:
\begin{enumerate}
    \item Nagrywanie obrazów RGB o rozdzielczości $224 \times 224$ pikseli
    \item Automatyczną detekcję dłoni za pomocą MediaPipe Hand Landmarker
    \item Odrzucenie próbek, w których nie wykryto dłoni
\end{enumerate}

\subsection{Charakterystyka zbioru}

\begin{itemize}
    \item \textbf{Liczba próbek}: 2549 (po filtracji MediaPipe)
    \item \textbf{Liczba klas}: 22 litery PSL (A-Z bez J, Q, V, X)
    \item \textbf{Rozkład klas}: quasi-zbalansowany, 101-119 próbek na klasę
    \item \textbf{Format}: Obrazy RGB $224 \times 224 \times 3$
    \item \textbf{Pominięte próbki}: ~294 (~10.3\%), MediaPipe nie wykrył dłoni
\end{itemize}

Rozkład klas przedstawia Tabela \ref{tab:class_dist}. Zbiór jest stosunkowo zbalansowany, co minimalizuje potrzebę stosowania technik ważenia klas.

\begin{table}[h]
\centering
\caption{Rozkład próbek w zbiorze danych}
\label{tab:class_dist}
\small
\begin{tabular}{lccccccc}
\toprule
\textbf{Klasa} & A & B & C & D & E & F & G \\
\textbf{Liczba} & 113 & 119 & 119 & 112 & 109 & 113 & 112 \\
\midrule
\textbf{Klasa} & H & I & K & L & M & N & O \\
\textbf{Liczba} & 117 & 116 & 119 & 119 & 119 & 117 & 119 \\
\midrule
\textbf{Klasa} & P & R & S & T & U & W & Y \\
\textbf{Liczba} & 115 & 119 & 101 & 115 & 119 & 119 & 119 \\
\midrule
\textbf{Klasa} & Z & & & & & & \\
\textbf{Liczba} & 119 & & & & & & \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Ekstrakcja cech - punkty charakterystyczne dłoni}

Dla każdego obrazu RGB, MediaPipe Hand Landmarker ekstrahuje 21 punktów charakterystycznych dłoni, każdy opisany przez współrzędne $(x, y, z)$. Aby zwiększyć odporność na translacje i skalowanie, zastosowano normalizację względem nadgarstka:

\begin{lstlisting}[caption={Ekstrakcja i normalizacja punktów charakterystycznych}]
def extract_landmarks(image):
    results = hands.process(cv2.cvtColor(image, 
                                         cv2.COLOR_BGR2RGB))
    if not results.multi_hand_landmarks:
        return None
    
    hand = results.multi_hand_landmarks[0]
    landmarks = [(lm.x, lm.y, lm.z) for lm in hand.landmark]
    
    wrist = np.array(landmarks[0])
    landmarks_norm = [np.array(lm) - wrist 
                      for lm in landmarks]
    
    return np.array(landmarks_norm).flatten()
\end{lstlisting}

Ostatecznie, każda próbka jest reprezentowana przez wektor 78-wymiarowy ($21 \times 3 = 63$ współrzędne + 15 relacji geometrycznych między punktami).

\section{Augmentacja danych}

Aby zwiększyć rozmiar zbioru treningowego i poprawić generalizację modeli, zastosowano augmentację danych. Ze względu na statyczny charakter gestów PJM oraz semantyczne znaczenie orientacji dłoni, wybrano następujące transformacje:

\begin{itemize}
    \item \textbf{Obrót}: $\pm 15^{\circ}$
    \item \textbf{Przesunięcie}: $\pm 10\%$ szerokości i wysokości
    \item \textbf{Skalowanie}: $90-110\%$ rozmiaru
    \item \textbf{Jasność}: $\pm 20\%$
    \item \textbf{Kontrast}: $\pm 20\%$
\end{itemize}

\subsection{Proces augmentacji}

Augmentacja została przeprowadzona w dwóch etapach, zgodnie z zasadą separacji generowania danych od treningu modeli:

\begin{enumerate}
    \item \textbf{Wstępne generowanie augmentowanych obrazów}: Napisano skrypt \texttt{generate\_augmented\_data.py}, który dla każdego oryginalnego obrazu generuje 5 augmentowanych wersji przy użyciu biblioteki Albumentations. Wygenerowane obrazy zapisywane są w katalogu \texttt{data/augmented/} z nazwami odpowiadającymi oryginalnym plikom (np. \texttt{A1\_aug0.jpg}, \texttt{A1\_aug1.jpg}, ...).
    
    \item \textbf{Dobieranie danych podczas treningu}: W czasie walidacji krzyżowej, dla każdej próbki należącej do zbioru treningowego danego foldu, ładowane są zarówno oryginalne, jak i odpowiadające im augmentowane obrazy. Z wszystkich tych obrazów ekstrahowane są punkty charakterystyczne, które następnie trafiają do zbioru treningowego. Dzięki temu augmentacja jest wykonywana raz, a następnie wielokrotnie wykorzystywana.
\end{enumerate}

Kluczowe jest, że zbiór walidacyjny zawsze składa się wyłącznie z oryginalnych próbek, aby zapewnić uczciwe porównanie modeli i uniknąć przecieku informacji między zbiorami.

\section{Metodologia eksperymentów}

\subsection{Stratified K-Fold Cross-Validation}

Wszystkie eksperymenty przeprowadzono z użyciem 5-krotnej stratyfikowanej walidacji krzyżowej. Stratyfikacja zapewnia, że każda klasa ma proporcjonalną reprezentację w każdym foldzie, co jest szczególnie istotne przy małych zbiorach danych.

Proces walidacji:
\begin{enumerate}
    \item Podział danych na 5 foldów ze stratyfikacją
    \item Dla każdego foldu:
    \begin{itemize}
        \item 4 foldy jako zbiór treningowy
        \item 1 fold jako zbiór walidacyjny
        \item Trening modelu na zbiorze treningowym
        \item Ewaluacja na zbiorze walidacyjnym
    \end{itemize}
    \item Agregacja wyników: średnia i odchylenie standardowe metryk
\end{enumerate}

\subsection{Modele klasyfikacyjne}

Zbadano cztery różne podejścia:

\subsubsection{Random Forest}

Ensemble 100 drzew decyzyjnych z maksymalną głębokością 20. Hiperparametry:
\begin{lstlisting}
RandomForestClassifier(
    n_estimators=100,
    max_depth=20,
    random_state=42
)
\end{lstlisting}

\subsubsection{Logistic Regression}

Wieloklasowa regresja logistyczna z regularyzacją L2:
\begin{lstlisting}
LogisticRegression(
    max_iter=1000,
    random_state=42,
    multi_class='multinomial'
)
\end{lstlisting}

\subsubsection{Keras MLP}

Wielowarstwowa sieć neuronowa z następującą architekturą:
\begin{lstlisting}
model = Sequential([
    Dense(256, activation='relu', input_dim=78),
    Dropout(0.3),
    Dense(128, activation='relu'),
    Dropout(0.3),
    Dense(64, activation='relu'),
    Dense(22, activation='softmax')
])
model.compile(
    optimizer='adam',
    loss='categorical_crossentropy'
)
\end{lstlisting}

Trenowanie przez 50 epok z early stopping (patience=10).

\subsubsection{CNN}

Ze względu na błąd implementacyjny, CNN nie został włączony do porównania augmentacji. Model operował bezpośrednio na obrazach RGB.

\subsection{Metryki ewaluacji}

Do oceny modeli wykorzystano następujące metryki:

\begin{itemize}
    \item \textbf{Accuracy}: Procent poprawnie sklasyfikowanych próbek
    \item \textbf{Balanced Accuracy}: Średnia recall per-class (uwzględnia niezbalansowanie)
    \item \textbf{F1-macro}: Średnia harmoniczna precision i recall, uśredniona po klasach
    \item \textbf{F1-weighted}: F1 ważone liczebnością klas
    \item \textbf{Precision/Recall per-class}: Analiza błędów dla każdej litery
    \item \textbf{Macierz pomyłek}: Wizualizacja błędnych klasyfikacji
\end{itemize}

\section{Wyniki eksperymentów}

\subsection{Eksperymenty bez augmentacji}

Tabela \ref{tab:results_baseline} przedstawia wyniki dla modeli trenowanych na oryginalnym zbiorze danych.

\begin{table}[h]
\centering
\caption{Wyniki eksperymentów bez augmentacji (5-fold CV)}
\label{tab:results_baseline}
\begin{tabular}{lcccc}
\toprule
\textbf{Model} & \textbf{Accuracy} & \textbf{Balanced Acc} & \textbf{F1-macro} & \textbf{F1-weighted} \\
\midrule
Random Forest & $0.9847 \pm 0.0038$ & $0.9848 \pm 0.0039$ & $0.9848 \pm 0.0039$ & $0.9847 \pm 0.0038$ \\
LogisticRegression & $0.9823 \pm 0.0061$ & $0.9823 \pm 0.0062$ & $0.9823 \pm 0.0061$ & $0.9823 \pm 0.0061$ \\
Keras MLP & $0.9839 \pm 0.0064$ & $0.9830 \pm 0.0072$ & $0.9839 \pm 0.0074$ & $0.9839 \pm 0.0065$ \\
\bottomrule
\end{tabular}
\end{table}

Wszystkie trzy modele osiągnęły bardzo wysoką dokładność ($>98\%$), co potwierdza skuteczność reprezentacji opartej na landmarkach. Random Forest wykazał najwyższą średnią accuracy oraz najmniejsze odchylenie standardowe, co świadczy o jego stabilności.

\subsection{Eksperymenty z augmentacją}

Tabela \ref{tab:results_augmented} przedstawia wyniki dla modeli trenowanych z augmentacją danych.

\begin{table}[h]
\centering
\caption{Wyniki eksperymentów z augmentacją danych (5-fold CV)}
\label{tab:results_augmented}
\begin{tabular}{lcccc}
\toprule
\textbf{Model} & \textbf{Accuracy} & \textbf{Balanced Acc} & \textbf{F1-macro} & \textbf{F1-weighted} \\
\midrule
Random Forest & $0.9887 \pm 0.0036$ & $0.9877 \pm 0.0056$ & $0.9875 \pm 0.0052$ & $0.9887 \pm 0.0036$ \\
LogisticRegression & $0.9847 \pm 0.0061$ & $0.9832 \pm 0.0082$ & $0.9823 \pm 0.0086$ & $0.9847 \pm 0.0061$ \\
Keras MLP & $0.9851 \pm 0.0042$ & $0.9845 \pm 0.0051$ & $0.9824 \pm 0.0053$ & $0.9851 \pm 0.0042$ \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Porównanie wpływu augmentacji}

Tabela \ref{tab:comparison} przedstawia bezpośrednie porównanie wyników z i bez augmentacji.

\begin{table}[h]
\centering
\caption{Wpływ augmentacji danych na wydajność modeli}
\label{tab:comparison}
\begin{tabular}{lccc}
\toprule
\textbf{Model} & \textbf{Baseline Acc} & \textbf{Augmented Acc} & \textbf{$\Delta$ Acc (\%)} \\
\midrule
Random Forest & $0.9847$ & $0.9887$ & $+0.40$ \\
LogisticRegression & $0.9823$ & $0.9847$ & $+0.24$ \\
Keras MLP & $0.9839$ & $0.9851$ & $+0.12$ \\
\bottomrule
\end{tabular}
\end{table}

Augmentacja danych przyniosła nieznaczną poprawę dla wszystkich modeli, przy czym największy wzrost odnotowano dla Random Forest ($+0.40\%$). Tak niewielki wpływ augmentacji wynika z już bardzo wysokiej bazowej dokładności modeli.

\subsection{Analiza per-class}

Tabela \ref{tab:perclass} przedstawia szczegółową analizę dla wybranych klas (Random Forest z augmentacją).

\begin{table}[h]
\centering
\caption{Metryki per-class dla Random Forest (augmented)}
\label{tab:perclass}
\small
\begin{tabular}{lcccc}
\toprule
\textbf{Klasa} & \textbf{Support} & \textbf{Precision} & \textbf{Recall} & \textbf{F1} \\
\midrule
A & 113 & 1.0000 & 1.0000 & 1.0000 \\
B & 119 & 0.9920 & 1.0000 & 0.9959 \\
D & 112 & 0.9819 & 0.9822 & 0.9816 \\
F & 113 & 0.9833 & 0.9911 & 0.9870 \\
O & 119 & 0.9524 & 0.9412 & 0.9456 \\
S & 101 & 1.0000 & 1.0000 & 1.0000 \\
T & 115 & 0.9833 & 0.9739 & 0.9778 \\
\bottomrule
\end{tabular}
\end{table}

Warto zauważyć, że proste klasyfikatory, dysponując wyekstrahowanymi punktami charakterystycznymi dłoni, radzą sobie doskonale. Wiele klas osiągnęło perfekcyjny wynik accuracy równy 1.0 (np. A, B, G, M, N, S, U, W, Y dla Logistic Regression).

\subsection{Macierze pomyłek}

Rysunek \ref{fig:confusion} przedstawia zagregowaną macierz pomyłek dla Random Forest z augmentacją. Większość próbek została sklasyfikowana poprawnie (koncentracja na diagonali).

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{../results_cv/confusion_randomforest_(augmented)_aggregate.png}
\caption{Macierz pomyłek dla Random Forest z augmentacją (agregowana z 5 foldów)}
\label{fig:confusion}
\end{figure}

Analiza macierzy pomyłek ujawnia, że największe trudności sprawiają litery o podobnej konfiguracji punktów charakterystycznych. Rysunek \ref{fig:confusion_lr} przedstawia szczegółową macierz dla Logistic Regression, gdzie widoczne są błędne klasyfikacje między podobnymi literami.

\newpage

\begin{figure}[t]
\centering
\includegraphics[width=0.8\textwidth]{../results_cv/confusion_logisticregression_aggregate.png}
\caption{Macierz pomyłek dla Logistic Regression. Szczególnie problematyczne pary to T-F oraz D-Z, których układy punktów charakterystycznych są do siebie bardzo podobne.}
\label{fig:confusion_lr}
\end{figure}

Dla litery T osiągnięto F1-score na poziomie 0.9184 (najniższy wynik wśród wszystkich klas w Logistic Regression). Błędne klasyfikacje między T a F wynikają z wysokiego podobieństwa układu palców - obie litery charakteryzują się złożoną konfiguracją palca wskazującego i kciuka.

\section{Wnioski}

\subsection{Główne obserwacje}

\begin{enumerate}
    \item \textbf{Skuteczność punktów charakterystycznych}: Reprezentacja oparta na punktach charakterystycznych MediaPipe okazała się bardzo efektywna, umożliwiając osiągnięcie dokładności przekraczającej 98\% przy użyciu prostych klasyfikatorów.
    
    \item \textbf{Random Forest jako najlepszy model}: Spośród badanych metod, Random Forest wykazał najwyższą dokładność ($98.87\%$ z augmentacją) i najlepszą stabilność (najmniejsze odchylenie standardowe).
    
    \item \textbf{Wpływ augmentacji}: Augmentacja danych przyniosła nieznaczną poprawę wyników wszystkich modeli ($+0.12$--$0.40\%$). Tak niewielki efekt wynika z już bardzo wysokich wyników bazowych - modele osiągnęły blisko optymalną wydajność na oryginalnych danych.
    
    \item \textbf{Proste a złożone modele}: Nie zaobserwowano znaczącej przewagi głębokich sieci neuronowych nad prostymi klasyfikatorami przy użyciu punktów charakterystycznych jako cech. Sugeruje to, że dla tego typu reprezentacji złożone modele nie są konieczne.
    
    \item \textbf{Problematyczne pary liter}: Analiza błędów klasyfikacji ujawniła, że największe trudności sprawiają litery o podobnej konfiguracji punktów (T-F, D-Z), co wskazuje na ograniczenia wynikające z podobieństwa gestów.
\end{enumerate}

\subsection{Ograniczenia badania}

\begin{itemize}
    \item \textbf{Statyczne gesty}: Badanie obejmowało tylko statyczne litery alfabetu, bez gestów dynamicznych.
    \item \textbf{Pojedynczy wykonawca}: Dane zebrano od jednej osoby, co mogło ograniczać generalizację.
    \item \textbf{Kontrolowane warunki}: Zdjęcia wykonano w podobnych warunkach oświetleniowych i z jednolitym tłem.
    \item \textbf{CNN bez augmentacji}: Błąd implementacyjny uniemożliwił pełne porównanie CNN z innymi metodami.
\end{itemize}

\subsection{Przyszłe kierunki badań}

\begin{enumerate}
    \item Rozszerzenie zbioru danych o więcej wykonawców i różnorodne warunki nagrywania
    \item Implementacja rozpoznawania gestów dynamicznych (słowa, zdania)
    \item Badanie transferu uczenia z wykorzystaniem pre-trenowanych modeli
    \item Optymalizacja dla zastosowań czasu rzeczywistego na urządzeniach mobilnych
    \item Integracja z systemem tłumaczenia kontekstowego PSL
\end{enumerate}

\section{Podsumowanie}

W pracy przeprowadzono szczegółowe badanie eksperymentalne nad klasyfikacją liter polskiego języka migowego. Wykazano, że reprezentacja oparta na punktach charakterystycznych dłoni w połączeniu z klasycznymi metodami uczenia maszynowego (w szczególności Random Forest) pozwalała osiągnąć bardzo wysoką dokładność klasyfikacji ($98.87\%$). Augmentacja danych przyczyniła się do dalszej poprawy wyników, choć jej wpływ był stosunkowo niewielki ze względu na już wysoką bazową dokładność modeli.

Rezultaty badań mogły stanowić podstawę dla praktycznych systemów wspomagających komunikację osób niesłyszących, przy czym konieczne było dalsze badanie w kierunku rozszerzenia na gesty dynamiczne oraz zwiększenia różnorodności zbioru danych.

\begin{thebibliography}{9}

\bibitem{lecun1998}
LeCun, Y., Bottou, L., Bengio, Y., Haffner, P.: Gradient-based learning applied to document recognition. Proceedings of the IEEE 86(11), 2278-2324 (1998)

\bibitem{krizhevsky2012}
Krizhevsky, A., Sutskever, I., Hinton, G.E.: ImageNet classification with deep convolutional neural networks. In: Advances in Neural Information Processing Systems 25 (NIPS 2012), pp. 1097-1105 (2012)

\bibitem{kolodziej2022}
Kołodziej, M., Szypuła, E., Majkowski, A., Rak, R.: Using deep learning to recognize the sign alphabet. Przegląd Elektrotechniczny 98(6) (2022). doi:10.15199/48.2022.06.06

\bibitem{ameen2017}
Ameen, S., Vadera, S.: A convolutional neural network to classify American Sign Language fingerspelling from depth and colour images. Expert Systems 34(3), e12197 (2017)

\bibitem{oguntimilehin2024}
Oguntimilehin, A., Balogun, K.: Real-Time Sign Language Fingerspelling Recognition using Convolutional Neural Network. International Arab Journal of Information Technology 21(1), 158-165 (2024)

\bibitem{piskozub2023}
Piskozub, J., Strumiłło, P.: Data Glove for the Recognition of the Letters of the Polish Sign Language Alphabet. In: The Latest Developments and Challenges in Biomedical Engineering, Proc. 23rd Polish Conference on Biocybernetics and Biomedical Engineering (PCBBE 2023), Springer, pp. 351-362 (2023)

\bibitem{filipowska2024}
Filipowska, A., Filipowski, W., Mieszczanin, J., Bryzik, K., Henkel, M., Skwarek, E., Raif, P., Sieciński, S., Doniec, R.J., Mika, B., et al.: Pattern Recognition in the Processing of Electromyographic Signals for Selected Expressions of Polish Sign Language. Sensors 24(18), 6710 (2024)

\bibitem{mediapipe}
Lugaresi, C., Tang, J., Nash, H., et al.: MediaPipe: A Framework for Building Perception Pipelines. arXiv preprint arXiv:1906.08172 (2019)

\end{thebibliography}

\end{document}
